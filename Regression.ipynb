{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "Simple Linear Regression is a statistical method used to model the relationship between a dependent variable and a single independent variable. It fits a straight line (Y = mX + c) through the data points that best explains the trend in the data."
      ],
      "metadata": {
        "id": "NyfTo9Mm7cUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "The key assumptions are linearity between variables, independence of errors, homoscedasticity (constant variance of errors), normality of residuals, and that there are no significant outliers influencing the model."
      ],
      "metadata": {
        "id": "GwTKlcr-7irp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y = mX + c?\n",
        "The coefficient m, also called the slope, represents the rate of change in the dependent variable Y for every one-unit increase in the independent variable X."
      ],
      "metadata": {
        "id": "dx0Ipcaj7khC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y = mX + c?\n",
        "The intercept c is the value of the dependent variable Y when the independent variable X is zero. It indicates the starting point of the regression line on the Y-axis."
      ],
      "metadata": {
        "id": "nTH81pES7mCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "The slope m is calculated using the least squares method, given by the formula:\n",
        "ùëö\n",
        "=\n",
        "ùëõ\n",
        "(\n",
        "‚àë\n",
        "ùëã\n",
        "ùëå\n",
        ")\n",
        "‚àí\n",
        "(\n",
        "‚àë\n",
        "ùëã\n",
        ")\n",
        "(\n",
        "‚àë\n",
        "ùëå\n",
        ")\n",
        "ùëõ\n",
        "(\n",
        "‚àë\n",
        "ùëã\n",
        "2\n",
        ")\n",
        "‚àí\n",
        "(\n",
        "‚àë\n",
        "ùëã\n",
        ")\n",
        "2\n",
        "m=\n",
        "n(‚àëX\n",
        "2\n",
        " )‚àí(‚àëX)\n",
        "2\n",
        "\n",
        "n(‚àëXY)‚àí(‚àëX)(‚àëY)\n",
        "‚Äã\n",
        " where n is the number of observations."
      ],
      "metadata": {
        "id": "zubbdmeF7oDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "The least squares method minimizes the sum of the squared differences between the observed values and the predicted values, ensuring the best-fitting regression line."
      ],
      "metadata": {
        "id": "WlUowJqY7rgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "R¬≤ indicates the proportion of variance in the dependent variable that can be explained by the independent variable. A higher R¬≤ means a better fit of the model to the data."
      ],
      "metadata": {
        "id": "z2QmAaOD7w4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "Multiple Linear Regression extends Simple Linear Regression by modeling the relationship between one dependent variable and two or more independent variables."
      ],
      "metadata": {
        "id": "M6ZTlCQg7ysI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "Simple Linear Regression uses one independent variable, while Multiple Linear Regression involves two or more independent variables to predict the dependent variable."
      ],
      "metadata": {
        "id": "1l-pTWgQ70Yh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "They include linearity, independence of errors, homoscedasticity, normality of residuals, and no multicollinearity among independent variables."
      ],
      "metadata": {
        "id": "W64HItQI73NK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "Heteroscedasticity refers to non-constant variance of errors across observations, which violates regression assumptions and can lead to inefficient estimates and biased inference."
      ],
      "metadata": {
        "id": "Peu9kpFl74rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "Multicollinearity can be addressed by removing highly correlated variables, using dimensionality reduction (like PCA), or applying regularization techniques like Ridge or Lasso regression."
      ],
      "metadata": {
        "id": "mx9H5tv875fB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "Common techniques include one-hot encoding, label encoding, and using dummy variables to convert categorical data into numerical form suitable for regression models."
      ],
      "metadata": {
        "id": "R9TENJvf77OV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "Interaction terms capture the effect of two variables acting together on the dependent variable, revealing combined influence that is not explained by individual effects."
      ],
      "metadata": {
        "id": "pRxvHK4d789r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "In Simple Linear Regression, the intercept represents the value of Y when X is zero. In Multiple Linear Regression, it shows the value of Y when all independent variables are zero, which might be less interpretable in real-world context."
      ],
      "metadata": {
        "id": "_NFZs2K77-gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "The slope quantifies the relationship strength and direction between an independent and dependent variable. A positive slope means a direct relationship, while a negative slope indicates an inverse relationship."
      ],
      "metadata": {
        "id": "zEnAYtGP8AVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "The intercept provides a baseline value of the dependent variable when all predictors are zero, offering a reference point for understanding the model's behavior."
      ],
      "metadata": {
        "id": "Pll9D5bb8Byi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "R¬≤ does not indicate whether the model is appropriate or if predictors are statistically significant. It also increases with more variables, even if they add little value, which can be misleading."
      ],
      "metadata": {
        "id": "FusVfh618DVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "A large standard error implies that the estimate of the coefficient is imprecise and there is high variability in its sampling distribution, possibly indicating unreliability in the variable's effect."
      ],
      "metadata": {
        "id": "vXSXGZnn8FEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "Heteroscedasticity appears as a funnel shape or pattern in residual plots. Addressing it is crucial because it violates regression assumptions and affects the accuracy of statistical tests."
      ],
      "metadata": {
        "id": "08Nvw65W8HiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "This suggests that additional predictors may not be significantly contributing to the model. Adjusted R¬≤ penalizes unnecessary variables, highlighting model overfitting."
      ],
      "metadata": {
        "id": "FKNg5_FJ8JRS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "Scaling ensures that all variables contribute equally to the model, especially when using regularization methods. It also helps avoid dominance of variables with large scales."
      ],
      "metadata": {
        "id": "qIPHq5Mq8K4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "Polynomial regression is an extension of linear regression where the relationship between the independent and dependent variable is modeled as an nth-degree polynomial."
      ],
      "metadata": {
        "id": "U196S1bo8MoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "Linear regression models straight-line relationships, while polynomial regression captures curvilinear relationships by adding powers of the independent variable."
      ],
      "metadata": {
        "id": "tRUccE3t8Ozw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?\n",
        "Polynomial regression is used when data shows a nonlinear relationship that cannot be captured by a straight line but can be fitted with a curved polynomial line."
      ],
      "metadata": {
        "id": "ZAqcjbx48QCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "The general form is:\n",
        "ùëå\n",
        "=\n",
        "ùëè\n",
        "0\n",
        "+\n",
        "ùëè\n",
        "1\n",
        "ùëã\n",
        "+\n",
        "ùëè\n",
        "2\n",
        "ùëã\n",
        "2\n",
        "+\n",
        "‚ãØ\n",
        "+\n",
        "ùëè\n",
        "ùëõ\n",
        "ùëã\n",
        "ùëõ\n",
        "Y=b\n",
        "0\n",
        "‚Äã\n",
        " +b\n",
        "1\n",
        "‚Äã\n",
        " X+b\n",
        "2\n",
        "‚Äã\n",
        " X\n",
        "2\n",
        " +‚ãØ+b\n",
        "n\n",
        "‚Äã\n",
        " X\n",
        "n\n",
        "\n",
        "where\n",
        "ùëõ\n",
        "n is the degree of the polynomial."
      ],
      "metadata": {
        "id": "up8MTSeq8R27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "Yes, polynomial regression can include multiple variables and their polynomial terms or interactions, though it increases model complexity."
      ],
      "metadata": {
        "id": "0ZkPN2kf8TzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?\n",
        "Yes, polynomial regression can include multiple variables and their polynomial terms or interactions, though it increases model complexity."
      ],
      "metadata": {
        "id": "pkGj1fDc8VUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "Cross-validation, adjusted R¬≤, AIC/BIC, and residual analysis can be used to select an optimal polynomial degree without overfitting."
      ],
      "metadata": {
        "id": "6TA2lMTl8W6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "Visualization helps in understanding the shape of the data and how well the polynomial curve fits it, making it easier to spot overfitting or underfitting."
      ],
      "metadata": {
        "id": "b_ntluvu8YsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "In Python, polynomial regression can be implemented using PolynomialFeatures from sklearn.preprocessing to generate polynomial terms, followed by fitting a linear regression model using LinearRegression."
      ],
      "metadata": {
        "id": "GXNnutkU8bWg"
      }
    }
  ]
}